{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def least_square(regressor, response):\n",
    "    # use numpy.linalg.lstsq\n",
    "    # modify residual and singular value, eliminate the effect caused by data size\n",
    "    if regressor.shape[0] <= regressor.shape[1]:\n",
    "        raise ValueError(\"regressor size error, #datums <= #variables\")\n",
    "    \n",
    "    coe, res, rank, singular_value = np.linalg.lstsq(regressor, response, rcond=-1)\n",
    "    rms = res / (regressor.shape[0] - regressor.shape[1]) # residual mean square\n",
    "    return coe, rms, rank, singular_value\n",
    "\n",
    "def pcvar(data):\n",
    "    # data : numpy matrix\n",
    "    # return Variance of each principal component\n",
    "    if data.shape[0] < 2:\n",
    "        raise ValueError(\"data size error, variance of single datum is meaningless\")\n",
    "    \n",
    "    data -= data.mean(axis=0)\n",
    "    output = np.linalg.eigvalsh(np.dot(data.T, data))[::-1]/(data.shape[0] - 1)\n",
    "    return output\n",
    "\n",
    "class LinearRegression():\n",
    "    def __init__(self, regressor, response, has_bias = True, regularizer = 10**-6):\n",
    "        self.set_data(regressor, response, has_bias)\n",
    "        self.set_regularizer(regularizer)\n",
    "    \n",
    "    def set_data(self, regressor, response, has_bias):\n",
    "        if (regressor.ndim != 2) or (regressor.size == 0):\n",
    "            raise ValueError(\"regressor should be a non-empty numpy matrix\")\n",
    "        elif (response.ndim != 2) or (response.size == 0):\n",
    "            raise ValueError(\"response should be a non-empty numpy matrix\")\n",
    "        elif len(regressor) != len(response):\n",
    "            raise ValueError(\"len(regressor) != len(response)\")\n",
    "        \n",
    "        self.num_var = regressor.shape[1]\n",
    "        self.has_bias = has_bias\n",
    "        if self.has_bias:\n",
    "            self.regressor = np.append(regressor, np.ones((len(regressor), 1)), axis=1)\n",
    "        else:\n",
    "            self.regressor = regressor\n",
    "        \n",
    "        self.response = response\n",
    "    \n",
    "    def set_regularizer(self, regularizer):\n",
    "        if regularizer < 0.:\n",
    "            self.regularizer = 0.\n",
    "            print(\"set_regularizer error, regularizer must be non-negative, has been set to zero.\")\n",
    "        else:\n",
    "            self.regularizer = regularizer\n",
    "    \n",
    "    def most_useless_analysis(self, drop=1):\n",
    "        gram = np.dot(self.regressor.T, self.regressor)\n",
    "        gram += self.regularizer * self.regressor.shape[0] * np.identity(self.regressor.shape[1])\n",
    "        projected = np.dot(self.regressor.T, self.response)\n",
    "        is_leave = np.ones((gram.shape[0]), dtype=bool)\n",
    "        response_square = np.square(self.response).sum(axis=0)\n",
    "        for d in range(drop):\n",
    "            square_err = np.inf * np.ones((gram.shape[0]), dtype=bool)\n",
    "            for i in range(self.num_var):\n",
    "                if is_leave[i]:\n",
    "                    work_index = (is_leave * (np.arange(len(is_leave)) != i)).astype(bool)\n",
    "                    coe = np.linalg.solve(gram[work_index][:, work_index], projected[work_index])\n",
    "                    square_err[i] = (response_square\n",
    "                                     - (coe * projected[work_index]).sum(axis=0) \n",
    "                                     - self.regularizer * np.square(coe).sum(axis=0)\n",
    "                                    ).sum()\n",
    "            \n",
    "            is_leave[np.argmin(square_err)] = False\n",
    "        \n",
    "        rms = square_err.min()/(self.regressor.shape[0] - is_leave.sum())\n",
    "        coe = np.linalg.solve(gram[is_leave][:, is_leave], projected[is_leave])\n",
    "        if self.has_bias:\n",
    "            return is_leave[:-1], coe[:-1], coe[-1].reshape(1, -1), rms\n",
    "        else:\n",
    "            return is_leave, coe, np.zeros((1, self.num_var)), rms\n",
    "    \n",
    "    def quick_drop_useless_one(self):\n",
    "        # Use PCA to find 1 useless variable\n",
    "        # It can't guarantee that finding the most useless 1\n",
    "        if self.has_bias:\n",
    "            linear_trans, _, _, _ = np.linalg.lstsq(self.regressor, self.response, rcond=-1)\n",
    "            centralized_data = self.regressor[:, :-1] - self.regressor[:, :-1].mean(axis=0)\n",
    "            cori, conp = np.linalg.eigh(np.dot(centralized_data.T, centralized_data))\n",
    "            cori /= self.regressor.shape[0] - self.regressor.shape[1]\n",
    "            error_bound = 1/(np.square(conp) + 0.000000001)\n",
    "            error_bound *= cori\n",
    "            error_bound = error_bound.min(axis=1)\n",
    "            error_bound *= np.linalg.norm(linear_trans[:-1], axis=1)**2\n",
    "            drop = error_bound.argmin()\n",
    "            \n",
    "            is_leave = (np.arange(self.regressor.shape[1]) != drop)\n",
    "            \n",
    "            gram = np.dot(self.regressor.T, self.regressor)\n",
    "            gram += self.regularizer * self.regressor.shape[0] * np.identity(self.regressor.shape[1])\n",
    "            projected = np.dot(self.regressor.T, self.response)\n",
    "            response_square = np.square(self.response).sum(axis=0)\n",
    "            \n",
    "            coe = np.linalg.solve(gram[is_leave][:, is_leave], projected[is_leave])\n",
    "            square_err = (response_square\n",
    "                             - (coe * projected[is_leave]).sum(axis=0) \n",
    "                             - self.regularizer * np.square(coe).sum(axis=0)\n",
    "                            ).sum()\n",
    "            \n",
    "            rms = square_err/(self.regressor.shape[0] - is_leave.sum())\n",
    "            return is_leave[:-1], coe[:-1], coe[-1].reshape(1, -1), rms\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Quick drop only work for case with bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.random.normal(0, 1, (1000000,4))\n",
    "prin = np.array([[0, 0, 1, 0], [0, 0, 0, 1], [0.8, 0.6, 0, 0], [0.6, -0.8, 0, 0]])\n",
    "data = np.dot(data, np.diag([2, 1.1, 1., 1.5]))\n",
    "data = np.dot(data, prin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.0016494 , 1.20928421, 2.2561086 , 4.00593953]),\n",
       " array([[ 7.98864243e-01,  1.05697489e-03, -6.01508457e-01,\n",
       "         -1.54313906e-03],\n",
       "        [ 6.01505663e-01,  2.88082748e-03,  7.98862325e-01,\n",
       "          1.27446690e-03],\n",
       "        [-4.66056718e-04, -3.82241702e-05,  1.94640575e-03,\n",
       "         -9.99997996e-01],\n",
       "        [-2.57724344e-03,  9.99995291e-01, -1.66553864e-03,\n",
       "         -4.02647432e-05]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.eigh(np.dot(data.T, data)/1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([False,  True,  True, False]),\n",
       " array([[-3.34473394e-01,  9.99999447e-01,  1.95554557e-09,\n",
       "         -5.94729810e-04],\n",
       "        [ 6.33484295e-04, -6.73787209e-10,  9.99999750e-01,\n",
       "          2.70446056e-05]]),\n",
       " array([[0.99833356, 1.999998  , 2.999997  , 4.00059338]]),\n",
       " 2.4632334031640264)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myLR = LinearRegression(data, data + np.array([1,2,3,4]))\n",
    "\n",
    "myLR.most_useless_analysis(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ True,  True,  True, False]),\n",
       " array([[ 9.99999201e-01, -2.69400676e-07, -3.47938420e-09,\n",
       "          8.92468671e-04],\n",
       "        [-2.66450012e-07,  9.99999357e-01,  7.91784128e-10,\n",
       "         -2.96222784e-04],\n",
       "        [ 2.99162715e-10, -5.03126056e-10,  9.99999750e-01,\n",
       "          2.64792406e-05]]),\n",
       " array([[0.999999  , 1.999998  , 2.999997  , 4.00059487]]),\n",
       " 1.209321589418169)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myLR.quick_drop_useless_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.set_default_tensor_type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def np2torch(numpy_array, torch_type = None):\n",
    "    if torch_type is None:\n",
    "        torch_type = torch.get_default_dtype()\n",
    "    return torch.from_numpy(numpy_array).type(torch_type)\n",
    "\n",
    "def to_32(x):\n",
    "    return x.type(torch.FloatTensor)\n",
    "\n",
    "def to_64(x):\n",
    "    return x.type(torch.DoubleTensor)\n",
    "\n",
    "def two_square_trace(A, B):\n",
    "    # find tr(AB)\n",
    "    # see https://github.com/Dogiko/Some-Tools\n",
    "    return A.view(1,-1).mm(B.t().contiguous().view(-1,1)).view(())\n",
    "\n",
    "def pos_gram(gram, regularlizer = None):\n",
    "    _type = gram.type()\n",
    "    _size = len(gram)\n",
    "    if regularlizer is None:\n",
    "        if gram.abs().max() == 0:\n",
    "            raise ValueError(\"gram error, expect matrix with none-zero element\")\n",
    "        \n",
    "        # the fraction of float32 is 2**(-23)~10**(-7) we start with 10**(-7) times of maximun element\n",
    "        regularlizer = gram.abs().max()*0.0000001\n",
    "    \n",
    "    if regularlizer <= 0:\n",
    "        raise ValueError(\"regularlizer error, expect positive, got %s\" %(regularlizer))\n",
    "    \n",
    "    while True:\n",
    "        lambdas, vectors = torch.symeig(gram + regularlizer*torch.eye(_size).type(_type))\n",
    "        if lambdas.min() > 0:\n",
    "            break\n",
    "        \n",
    "        regularlizer *= 2.\n",
    "    \n",
    "    return gram + regularlizer*torch.eye(_size).type(_type)\n",
    "\n",
    "class LinearRefiner():\n",
    "    def __init__(self, linearModel, regularizer = None):\n",
    "        self.set_model(linearModel)\n",
    "        self.set_regularizer(regularizer)\n",
    "    \n",
    "    def set_model(self, linearModel):\n",
    "        if type(linearModel) != torch.nn.Linear:\n",
    "            raise TypeError(\"linearModel type error, got, expect torch.nn.Linear\")\n",
    "        \n",
    "        self.model_weight = linearModel.weight.data\n",
    "        self.model_bias = linearModel.bias.data\n",
    "        self.in_features = linearModel.in_features\n",
    "        self.reset()\n",
    "    \n",
    "    def set_regularizer(self, regularizer):\n",
    "        self.regularizer = regularizer\n",
    "    \n",
    "    def reset(self):\n",
    "        self.datums_acc = 0\n",
    "        self.gram = torch.zeros((self.in_features+1, self.in_features+1))\n",
    "    \n",
    "    def data_input(self, data):\n",
    "        datums = data.size()[0]\n",
    "        expand = torch.cat((data, torch.ones((datums, 1))), 1)\n",
    "        self.gram += torch.mm(expand.t(), expand)\n",
    "        self.datums_acc += datums\n",
    "    \n",
    "    def drop_error(self, estimate = False):\n",
    "        # return errors of drop each regressor\n",
    "        # if estimate = True, using PCA get upper-bound of errors\n",
    "        expand_model = to_64(torch.cat((self.model_weight, self.model_bias.view(-1,1)), 1).t())\n",
    "        mean_gram = pos_gram(to_64(self.gram / self.datums_acc), self.regularizer) # avoid singular gram\n",
    "        res_ms = two_square_trace(expand_model.mm(expand_model.t()), mean_gram) # mean of square sum of y = xA\n",
    "        if estimate:\n",
    "            unbiased_gram = pos_gram(mean_gram[:-1, :-1] - torch.mm(mean_gram[:-1, -1:], mean_gram[-1:, :-1]))\n",
    "            cori, comp = torch.symeig(unbiased_gram, eigenvectors=True) # PCA\n",
    "            error_bound = 1/(0.0000001 + comp**2) # 0.0000001 for stability, avoid divide zero\n",
    "            error_bound *= cori\n",
    "            error_bound = error_bound.min(1)[0]*((expand_model[:-1]**2).sum(1)) # square-norm for weight without bias\n",
    "            \n",
    "            error_bound = to_32(error_bound)\n",
    "            res_ms = to_32(res_ms)\n",
    "            return error_bound, res_ms\n",
    "        else:\n",
    "            errors = to_64(torch.zeros((len(mean_gram)-1)))\n",
    "            _idx = list(range(self.in_features + 1))\n",
    "            for p in range(len(mean_gram)-1):\n",
    "                drop_p_idx = _idx[:p] + _idx[p+1:]\n",
    "                subgram = mean_gram[drop_p_idx][:, drop_p_idx]\n",
    "                new_model = torch.gesv(mean_gram[drop_p_idx, : ].mm(expand_model), subgram)[0]\n",
    "                errors[p] = res_ms - two_square_trace(new_model.mm(new_model.t()), subgram)\n",
    "            \n",
    "            errors = to_32(errors)\n",
    "            res_ms = to_32(res_ms)\n",
    "            return errors, res_ms\n",
    "    \n",
    "    def refine(self, drop = 1, quick = True):\n",
    "        if self.datums_acc == 0:\n",
    "            raise ValueError(\".datums_acc == 0, must input data before refine.\")\n",
    "        \n",
    "        leave_idx = list(range(self.in_features + 1)) # +1 for bias\n",
    "        expand_model = to_64(torch.cat((self.model_weight, self.model_bias.view(-1,1)), 1).t())\n",
    "        mean_gram = pos_gram(to_64(self.gram / self.datums_acc), self.regularizer)\n",
    "        # avoid singular gram\n",
    "        res_ms = two_square_trace(expand_model.mm(expand_model.t()), mean_gram) # mean of square sum of y = xA\n",
    "        if quick:\n",
    "            new_model = expand_model.clone()\n",
    "            for d in range(drop):\n",
    "                subgram = mean_gram[leave_idx][:, leave_idx]\n",
    "                unbiased_subgram = pos_gram(subgram[:-1, :-1] - torch.mm(subgram[:-1, -1:], subgram[-1:, :-1]))\n",
    "                cori, comp = torch.symeig(unbiased_subgram, eigenvectors=True) # PCA\n",
    "                error_bound = 1/(0.000000000000001 + comp**2) # 0.000000000000001 for stability, avoid divide zero\n",
    "                error_bound *= cori\n",
    "                error_bound = error_bound.min(1)[0]*((new_model[:-1]**2).sum(1)) # square-norm for weight without bias\n",
    "                pos = error_bound.min(0)[1] # argmin for bound, position for variable should be remove in leave index\n",
    "                leave_pos = list(range(len(leave_idx))) # leave_idx for leave_idx\n",
    "                leave_pos.remove(pos)\n",
    "                drop_p_gram = subgram[leave_pos][:, leave_pos]\n",
    "                # ridge regrassion : linear regression with regularlizer, guaranty existence of only one minimun\n",
    "                new_model = torch.gesv(subgram[leave_pos, : ].mm(new_model), drop_p_gram)[0]\n",
    "                leave_idx = leave_idx[:pos] + leave_idx[pos+1:] # update leave_idx\n",
    "            \n",
    "            mse = res_ms - two_square_trace(new_model.mm(new_model.t()), mean_gram[leave_idx][:, leave_idx])\n",
    "        else:\n",
    "            for d in range(drop):\n",
    "                leave_err = torch.zeros((len(leave_idx)-1))\n",
    "                for p in range(len(leave_idx)-1):\n",
    "                    drop_p_idx = leave_idx[:p] + leave_idx[p+1:]\n",
    "                    drop_p_gram = mean_gram[drop_p_idx][:, drop_p_idx]\n",
    "                    new_model = torch.gesv(mean_gram[drop_p_idx, : ].mm(expand_model), drop_p_gram)[0]\n",
    "                    leave_err[p] = res_ms - two_square_trace(new_model.mm(new_model.t()), drop_p_gram)\n",
    "                \n",
    "                mse, pos = leave_err.min(0)\n",
    "                leave_idx = leave_idx[:pos] + leave_idx[pos+1:]\n",
    "            \n",
    "            subgram = mean_gram[leave_idx][:, leave_idx]\n",
    "            new_model = torch.gesv(mean_gram[leave_idx, : ].mm(expand_model), subgram)[0]\n",
    "        \n",
    "        # mean square error\n",
    "        \n",
    "        new_model = to_32(new_model)\n",
    "        mse = to_32(mse)\n",
    "        res_ms = to_32(res_ms)\n",
    "        return leave_idx[:-1], new_model[:-1].t(), new_model[-1].view((-1)), mse, res_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2], tensor([[ 1.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  1.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  1.0000],\n",
       "         [-0.0005, -0.0004, -0.0000]]), tensor([1.0000, 2.0000, 3.0000, 4.0000]), tensor(1.2077), tensor(38.4530))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np2torch(np.random.normal(0, 1, (1000000, 4)))\n",
    "prin = np2torch(np.array([[0, 0, 1, 0], [0, 0, 0, 1], [0.8, 0.6, 0, 0], [0.6, -0.8, 0, 0]]))\n",
    "data = data.mm(torch.diag(torch.Tensor([2., 1.1, 1., 1.5]))).mm(prin)\n",
    "linear = torch.nn.modules.Linear(4, 4, bias=True)\n",
    "linear.weight.data = torch.eye(4)\n",
    "linear.bias.data = torch.Tensor([1, 2, 3, 4])\n",
    "my_linear_refiner = LinearRefiner(linear)\n",
    "my_linear_refiner.data_input(data)\n",
    "\n",
    "my_linear_refiner.refine(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([1.2482, 1.5515, 3.9893, 1.2077]), tensor(38.4530))\n",
      "(tensor([1.5591, 2.7817, 3.9893, 1.2077]), tensor(38.4530))\n",
      "tensor(0.0314)\n"
     ]
    }
   ],
   "source": [
    "print(my_linear_refiner.drop_error())\n",
    "print(my_linear_refiner.drop_error(estimate=True))\n",
    "print(my_linear_refiner.refine(1)[3]/(linear(data).data**2).mean(0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2], tensor([[-0.3330,  0.0009],\n",
       "         [ 1.0000,  0.0000],\n",
       "         [ 0.0000,  1.0000],\n",
       "         [-0.0002, -0.0000]]), tensor([1.0009, 2.0000, 3.0000, 4.0000]), tensor(2.4559), tensor(38.4530))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_linear_refiner.refine(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2], tensor([[ 1.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  1.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  1.0000],\n",
       "         [-0.0005, -0.0004, -0.0000]]), tensor([1.0000, 2.0000, 3.0000, 4.0000]), tensor(1.2077), tensor(38.4530))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_linear_refiner.refine(1, quick=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2], tensor([[ 1.0000e+00,  4.1369e-17, -1.0871e-19],\n",
       "         [-7.6685e-17,  1.0000e+00, -2.1742e-19],\n",
       "         [-5.7854e-20, -1.3976e-19,  1.0000e+00],\n",
       "         [-5.2154e-04, -3.6115e-04, -3.1210e-05]]), tensor([1.0000, 2.0000, 3.0000, 4.0000]), tensor(1.2077), tensor(38.4530))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_linear_refiner.reset()\n",
    "my_linear_refiner.data_input(data[:1000])\n",
    "my_linear_refiner.data_input(data[1000:])\n",
    "\n",
    "my_linear_refiner.refine(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.random.normal(0, 1, (10000,5))\n",
    "b = np.random.normal(0, 1, (2))\n",
    "A = np.random.normal(0, 1, (2,5))\n",
    "y = np.dot(x, A.T) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ True,  True,  True,  True, False]), array([[-0.47370461,  1.0729814 ],\n",
       "        [ 0.63462901, -0.30281068],\n",
       "        [-1.15664563,  0.55470661],\n",
       "        [ 0.56363912,  0.4343164 ]]), array([[ 0.40931801, -0.46809887]]), 0.03877764965096857)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myLR = LinearRegression(x, y)\n",
    "\n",
    "myLR.quick_drop_useless_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ True, False,  True,  True, False]), array([[-0.47752537,  1.07480446],\n",
       "        [-1.16679318,  0.55954847],\n",
       "        [ 0.56126137,  0.43545093]]), array([[ 0.41305917, -0.46988396]]), 0.5363484826459067)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myLR.most_useless_analysis(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3], tensor([[-0.4737,  0.6346, -1.1566,  0.5636],\n",
       "         [ 1.0730, -0.3028,  0.5547,  0.4343]]), tensor([ 0.4093, -0.4681]), tensor(0.0388), tensor(4.5471))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = torch.nn.modules.Linear(5,2,bias=True)\n",
    "linear.weight.data = np2torch(A)\n",
    "linear.bias.data = np2torch(b)\n",
    "my_linear_refiner = LinearRefiner(linear)\n",
    "my_linear_refiner.data_input(np2torch(x))\n",
    "\n",
    "my_linear_refiner.refine(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2], tensor([[-0.4772,  0.6325, -1.1547],\n",
       "         [ 1.0703, -0.3045,  0.5562]]), tensor([ 0.4108, -0.4669]), tensor(0.5540), tensor(4.5471))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_linear_refiner.refine(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 2, 3], tensor([[-0.4775, -1.1668,  0.5613],\n",
       "         [ 1.0748,  0.5595,  0.4355]]), tensor([ 0.4131, -0.4699]), tensor(0.5361), tensor(4.5471))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_linear_refiner.refine(2, quick=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Differen between PCA and Linear Regrassion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "foo = np.zeros((10,2))\n",
    "foo[:8] = np.dot(0.5*(np.arange(8) - 3.5).reshape(8,1), np.array([[0.8, 0.6]]))\n",
    "foo[8:] = np.dot((np.arange(2) - 0.5).reshape(2,1), np.array([[-0.6, 0.8]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUQQAACSKAACARBEAAJCo/wefsgxtTecmywAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111761b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(foo[:,0], foo[:,1], \"bp\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11180dd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x2y = np.linalg.lstsq(foo[:,:1], foo[:,1:], -1)[0][0,0]\n",
    "y2x = np.linalg.lstsq(foo[:,1:], foo[:,:1], -1)[0][0,0]\n",
    "\n",
    "plt.plot(foo[:,0], foo[:,0]*0.75, \"r\")\n",
    "plt.plot(foo[:,0], foo[:,0]*x2y, \"b\")\n",
    "plt.plot(foo[:,1]*y2x, foo[:,1], \"g\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 3.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo = torch.Tensor([2,3])\n",
    "fooo = foo.clone()\n",
    "fooo[1] = 10\n",
    "foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(3).type(torch.eye(2).type(torch.DoubleTensor).type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
